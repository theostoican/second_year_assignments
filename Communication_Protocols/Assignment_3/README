/*
	Theodor Stoican
	Tema 4 PC
	323CA
*/

Tema4 a fost destul de interesanta (conceptual si practic), avand in 
vedere ca nu stiam (inainte sa ma apuc) ce este un crawler web. Practic,
m-am apucat de implementat dupa ce-am citit putin despre crawlere web, pentru
a sti exact ce sunt si ce fac acestea. Date fiind acestea, o sa redau, in cele
de mai jos, cateva detalii de implementare:
*Am folosit C++ pentru implementarea temei, intrucat o implementare in C++ mi
s-a parut mai facila decat una in C, partial datorita STL-ului. Recunosc ca
nu am folosit exceptii pentru fisierele log, dat fiind ca nu stiu sa lucrez
cu exceptii in C++(doar in Java).
*Conectarea la serverul HTTP si descarcarea paginilor o realizez similar cu 
exercitiile de la laborator, iar protocolul de transmitere a datelor de la client
la server este adaptat in functie de context. Se trimit pachete de 4096 octeti,
iar cand trimit un pachet, ii spun serverului sa asculte, trimitandu-i serverului
un mesaj "listen", dupa care dimensiunea pachetului (pentru date binare), calea 
(pentru crearea fisierului) si in final datele. Tranzactia se termina cu un "ACK"
trimis din partea serverului. Serverul permite multiplexare (pentru client si input)
si isi incheie executia dupa o comanda de tip download-asa cum se cere in enunt.
*Ca dificultati de implementare, am avut probleme cu fisierele de tip private,
care generau erori cand dadeam GET pe serverul HTTP.
*Cand a trebuit sa descarc anumite fisiere binare(.jpg, .gif), a fost nevoie sa modific
structura programului (initial foloseam fprintf, dupa care am schimbat cu fwrite, pentru
a putea permite scrierea datelor binare).
*In ceea ce priveste parser-ul, in cadrul clientului, ma ocup mai intai, intr-un bloc if, 
de linkurile cu extensia .htm/.html, iar apoi daca variabila de stare everything este 
setata, voi procesa si celelalte link-uri. Conform protocolului, trimit mai intai
mesajul catre server, dupa care, daca variabila de stare recursive este si ea setata,
trimit si lista cu celelalte link-uri care trebuie descarcate. La server, pastrez
totul intr-o lista dupa ce primesc si distribui uniform clientilor conform unui 
vector, buckets. De asemenea, in cazul in care unele pagini contin un link care
a fost deja descarcat, atunci voi verifica existenta fisierului aferent la 
nivelul server-ului si voi cere clientilor sa descarce doar daca fisierul nu
exista. 

Cam acestea ar fi, in mare, cateva detalii relevante relative la implementare.
Sper ca este o descriere decenta.

Theodor
